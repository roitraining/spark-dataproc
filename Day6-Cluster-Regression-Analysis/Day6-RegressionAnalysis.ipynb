{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ng2_--H6q0C_"
   },
   "source": [
    "### Initialize the spark environment and load the helper functions we have provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "itiXd2lDq0DD"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "rootpath = '/class/'\n",
    "datapath = f'{rootpath}datasets/'\n",
    "sys.path.append(rootpath)\n",
    "import pyspark_helpers as pyh\n",
    "from pyspark_helpers import *\n",
    "sc, spark, conf = initspark()\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib as mp\n",
    "import numpy\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from pyspark_helpers import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G73tQ_vxq0DJ"
   },
   "source": [
    "### Read in a simple dataset of Boston Housing Prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ppQVsqVLq0DL",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename = 'boston.csv'\n",
    "df = spark.read.csv(f'{datapath}/finance/{filename}', header = True, inferSchema = True)\n",
    "display(df)\n",
    "df.printSchema()\n",
    "\n",
    "# Save a pointer to the raw data\n",
    "dfRaw = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Be3ExXpnSMh0"
   },
   "outputs": [],
   "source": [
    "print(df.count())\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Co2UqS4RYtfW"
   },
   "source": [
    "### Let's look at the result of StringIndex to understand how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Zm7_7wFq0DP",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "col = 'TOWN'\n",
    "indexer = StringIndexer(inputCol = col, outputCol = col+'_Index')\n",
    "x1 = indexer.fit(df).transform(df)\n",
    "\n",
    "display(x1.select(col, col+'_Index').distinct().orderBy(col))\n",
    "display(x1.select(col, col+'_Index').distinct().orderBy(col+'_Index'))\n",
    "\n",
    "display(x1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lvdflq9KYtfb"
   },
   "source": [
    "### Now try it with a convenient helper function we wrote to encode a list of multiple columns automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7_7AYzbiq0DS"
   },
   "outputs": [],
   "source": [
    "x2 = pyh.StringIndexEncode(df, ['TOWN', 'TRACT'])\n",
    "display(x2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oFqX04_JYtfh"
   },
   "source": [
    "### Let's take a look at how OneHotEncoder works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yhilC-8Yq0DX"
   },
   "outputs": [],
   "source": [
    "col = 'TOWN'\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "\n",
    "encoder = OneHotEncoderEstimator(inputCols=[col + '_Index'], outputCols=[col+'_Vector'])\n",
    "display(encoder.fit(x2).transform(x2).orderBy(col + '_Index'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KUZJ4tFzYtfm"
   },
   "source": [
    "### Now try our convenient helper function. Note that it automatically called StringIndexer first so we can work on the raw string version of the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TloL3VnOYtfn"
   },
   "outputs": [],
   "source": [
    "x = pyh.OneHotEncode(x2, ['TOWN', 'TRACT'])\n",
    "display (x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WoS6pO-gYtfq"
   },
   "source": [
    "### Let's have a look at the Median Value, which is the target we want to predict.\n",
    "Spark does not have plotting of it's own, instead we bring back the data to the driver to plot. So we need to make sure not to bring back more than the driver can handle.\n",
    "The .toPandas method will act like collect but bring it back as a Pandas DataFrame which is easily plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HAgDBLRwq0Db"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "#sns.distplot(df.toPandas()['MEDV'])\n",
    "sns.distplot(df.select('MEDV').toPandas())\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lh26OC3LYtfw"
   },
   "source": [
    "### There's some outlier data there past 48, so let's just eliminate it for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KDt6wIXJYtfx"
   },
   "outputs": [],
   "source": [
    "#sns.distplot(df.where('MEDV < 48').toPandas()['MEDV'])\n",
    "sns.distplot(df.where('MEDV < 48').select('MEDV').toPandas())\n",
    "print(df.columns)\n",
    "\n",
    "# If we want to filter out the outliers\n",
    "dfRaw = dfRaw.where('MEDV < 48')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U7n8tW2GYtf1"
   },
   "source": [
    "### Let's put it all together now. Identify the categorical and numeric features and target.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cDJ269zdq0De"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "numeric_features = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO']\n",
    "categorical_features = ['TOWN'] #['TOWN', 'TRACT']\n",
    "target_label = 'MEDV'\n",
    "\n",
    "vecAssembler = VectorAssembler(inputCols= numeric_features , outputCol=\"features\")\n",
    "dfML = vecAssembler.transform(df)\n",
    "display(dfML)\n",
    "\n",
    "\n",
    "# df = dfRaw.select(categorical_features + numeric_features + [target_label])\n",
    "# df.printSchema()\n",
    "\n",
    "# print ('******')\n",
    "# display(df.describe())\n",
    "\n",
    "# print ('******')\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "egQIxlV9q0Di"
   },
   "source": [
    "### Turn the DataFrame into vectors.\n",
    "Use our MakeMLDataFrame helper function to automatically encode the list of categorical values, and bundle everything up into a features and target vector as needed for ML training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yQ0GoRKUYtf7"
   },
   "outputs": [],
   "source": [
    "dfML = pyh.MakeMLDataFrame(df, categorical_features, numeric_features, target_label, False)\n",
    "display(dfML)\n",
    "dfML.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kaAFz43aq0Do"
   },
   "source": [
    "### Split the dataset into train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6E281O5cq0Dp"
   },
   "outputs": [],
   "source": [
    "train, test = dfML.randomSplit([.7,.3], seed = 1000)\n",
    "print (f'Training set row count {train.count()}')\n",
    "print (f'Testing set row count {test.count()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dHewcp-Kq0Dv"
   },
   "source": [
    "### Run Linear Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w7meyIk2q0Dw"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(featuresCol = 'features', labelCol='target', maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lrModel = lr.fit(train)\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))\n",
    "\n",
    "print(\"Root Mean Squared Error: {}\\nR Squared (R2) {}\".format(lrModel.summary.rootMeanSquaredError, lrModel.summary.r2))\n",
    "\n",
    "print(dir(lrModel.summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "45d8vng4SMiz"
   },
   "outputs": [],
   "source": [
    "dir(lrModel.write)\n",
    "lrModel.write().overwrite().save('LinearRegression_trainedModel')\n",
    "print('LR Saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L8332MMmSMi3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gDuwpgTkq0Dz"
   },
   "source": [
    "### Run test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bPWNhVvxq0D1"
   },
   "outputs": [],
   "source": [
    "lrPredictions = lrModel.transform(test)\n",
    "display(lrPredictions.select(\"prediction\",\"target\",\"features\"), 30)\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "lrEvaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"target\",metricName=\"r2\")\n",
    "testResult = lrModel.evaluate(test)\n",
    "print(\"Root Mean Squared Error on Test set: {}\".format(testResult.rootMeanSquaredError))\n",
    "print(testResult.r2, testResult.r2adj)\n",
    "print(dir(testResult))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vBJooYD3q0D5"
   },
   "source": [
    "### Try Decision Tree Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YCP7q5wdq0D7"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "dt = DecisionTreeRegressor(featuresCol ='features', labelCol = 'target')\n",
    "dtModel = dt.fit(train)\n",
    "dtPredictions = dtModel.transform(test)\n",
    "display(dtPredictions.select(\"prediction\",\"target\",\"features\"), 30)\n",
    "important = dtModel.featureImportances\n",
    "print(type(important), important)\n",
    "#importantDict = zip(important[1], important[2])\n",
    "#print (importantDict)\n",
    "print (important[3])\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "dtEvaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"target\",metricName=\"rmse\")\n",
    "testResult = dtEvaluator.evaluate(dtPredictions)\n",
    "print(\"Root Mean Squared Error: {}\".format(testResult))\n",
    "dfML.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YRwC0NLEq0D-"
   },
   "source": [
    "### Try Gradient Boosted Tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lmcJ4_trq0EA"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "gbt = GBTRegressor(featuresCol = 'features', labelCol = 'target', maxIter=10)\n",
    "gbtModel = gbt.fit(train)\n",
    "gbtPredictions = gbtModel.transform(test)\n",
    "display(gbtPredictions.select('prediction', 'target', 'features'), 20)\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "gbtEvaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"target\",metricName=\"rmse\")\n",
    "testResult = gbtEvaluator.evaluate(gbtPredictions)\n",
    "print(\"Root Mean Squared Error: {}\".format(testResult))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rxEOP5ubq0EE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Ch05_RegressionAnalysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
