{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_SrjR77fw4JW"
   },
   "source": [
    "### Do the normal setup to get Spark context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8hEvV-LUw4JY",
    "outputId": "49fb7fe3-a0fb-4882-ce4e-1868cd70827d"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "rootpath = '/home/student/ROI/SparkProgram/'\n",
    "datapath = f'{rootpath}datasets/'\n",
    "sys.path.append(rootpath)\n",
    "import pyspark_helpers as pyh\n",
    "from pyspark_helpers import *\n",
    "sc, spark, conf = initspark()\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib as mp\n",
    "import numpy\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from pyspark_helpers import display\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, lit\n",
    "from functools import reduce\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud \n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pu96rJogw4Jd"
   },
   "source": [
    "### Read the Alexa reviews sample dataset from Kaggle.\n",
    "#### https://www.kaggle.com/sid321axn/amazon-alexa-reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yF4lDKVZw4Je",
    "outputId": "02f99c12-9a78-4e98-ed84-51531f9398b2"
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"rating\", IntegerType(), True),\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"variation\", StringType(), True),\n",
    "    StructField(\"verified_reviews\", StringType(), True),\n",
    "    StructField(\"feedback\", IntegerType(), True)])\n",
    "df = spark.read.option(\"delimiter\",\"\\t\").schema(schema).option(\"inferSchema\", \"True\").csv(\"amazon_alexa.tsv\")\n",
    "display(df)\n",
    "df.createOrReplaceTempView('alexa')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EGctVHIsw4Ji"
   },
   "source": [
    "### Dealing with unstructured data works better at the RDD level because we will need to use a lot of custom functions using the map method. So let's just select the reviews column. \n",
    "\n",
    "### The flatMap returning the object itself is a trick to turn the RDD from containing Row objects into regular string objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_PAPMM3ew4Jj",
    "outputId": "00b40a3e-001b-459f-96ab-c729d2596e18"
   },
   "outputs": [],
   "source": [
    "df = spark.sql(\"select lower(verified_reviews) from alexa where verified_reviews is not null and verified_reviews <> 'verified_reviews'\")\n",
    "# Note how each row is returned as a row element\n",
    "print(df.rdd.take(1))\n",
    "\n",
    "# flatMap converts the row into a regular string, because there is only one column in each row\n",
    "# if the dataframe had two columns in it, this would not work\n",
    "reviewsRDD = df.rdd.flatMap(lambda x: x)\n",
    "print(df.count(), reviewsRDD.count())\n",
    "print(reviewsRDD.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y8FuOlEfw4Jm"
   },
   "source": [
    "### nltk has a function to split text up into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4hjYoJw1w4Jn",
    "outputId": "0dd408e9-5309-45c4-9355-4573ff79e8ac"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# punctuation tokenizer, needs to be downloaded at least once. \n",
    "# make sure not to put this inside a loop or it will call it many times and slow down performance\n",
    "nltk.download('punkt')\n",
    "sentenceTokenizeRDD = reviewsRDD.map(lambda x : nltk.sent_tokenize(x))\n",
    "sentenceTokenizeRDD.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DJJ_zUfhw4Js"
   },
   "source": [
    "### Once the reviews are broken into sentences, let's break each sentence into a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n4hd-yJHw4Jt",
    "outputId": "75e1907e-ece8-4b40-b0e4-57cdd9268e2e"
   },
   "outputs": [],
   "source": [
    "wordTokenizeRDD = sentenceTokenizeRDD.map(lambda x : [word for line in x for word in line.split()])\n",
    "wordTokenizeRDD.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6IwMm1ccw4Jx"
   },
   "source": [
    "### Next, remove stop words, punctuation, and empty spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ONhb2dSEw4Jy",
    "outputId": "fcb4a802-62a6-444e-b79c-99ccc126df5f"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "# Must also make sure to download the stopwords list at least once.\n",
    "# Also be sure not to put this inside a loop\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "def removePunctuations(x):\n",
    "    list_punct = list(string.punctuation)\n",
    "    filtered = [''.join(c for c in s if c not in list_punct) for s in x] \n",
    "    filtered_space = [s for s in filtered if s] #remove empty space \n",
    "    return filtered\n",
    "\n",
    "def removeStopWords(x, language = 'english', additional_words = {}):\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words=set(stopwords.words(language)).union(additional_words)\n",
    "    filteredSentence = [w for w in x if not w in stop_words]\n",
    "    return filteredSentence\n",
    "\n",
    "nopunctRDD = wordTokenizeRDD.map(removePunctuations)\n",
    "\n",
    "# I decided to add a few extra stop words to the list just to show how easy it is\n",
    "stopwordsRDD = nopunctRDD.map(lambda x : removeStopWords(x, 'english', {'u', 'r', 'im', 'ive'}))\n",
    "stopwordsRDD.take(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TS_lXOmRw4J1"
   },
   "source": [
    "### wordnet is a library that has rules of the English language and how to parse it to standardize tense and case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VQHOwTSUw4J2",
    "outputId": "f76f4cc7-88b1-4478-facd-9a98734bb5d6"
   },
   "outputs": [],
   "source": [
    "# download at least once, but not inside a loopo\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def lemmatize(x):\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    finalLem = [lemmatizer.lemmatize(s) for s in x]\n",
    "    return finalLem\n",
    "\n",
    "lemwordsRDD = stopwordsRDD.map(lemmatize)\n",
    "lemwordsRDD.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DGA2QsZEw4J7"
   },
   "source": [
    "### After splitting it into individual words to fix the words, let's put it back together as a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WPfXVua5w4J8",
    "outputId": "c35c7e17-4b95-4489-ea61-6d4cee09caf5"
   },
   "outputs": [],
   "source": [
    "def joinTokensFunct(x):\n",
    "    joinedTokens_list = []\n",
    "    x = \" \".join(x)\n",
    "    return x\n",
    "joinedTokens = lemwordsRDD.map(joinTokensFunct)\n",
    "joinedTokens.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mLaZunv6w4KA"
   },
   "source": [
    "### Perceptron tagger goes through and adds additional information on words by adding things like part of speech.\n",
    "\n",
    "### Finds combinations of words that belong together to be treated as a phrase instead of individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j_GSsLl-w4KB",
    "outputId": "626d925f-4045-4ea1-8397-902ebfc66764"
   },
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def extractPhrase(x):\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    def leaves(tree):\n",
    "        \"\"\"Finds NP (nounphrase) leaf nodes of a chunk tree.\"\"\"\n",
    "        for subtree in tree.subtrees(filter = lambda t: t.label()=='NP'):\n",
    "            yield subtree.leaves()\n",
    "    \n",
    "    def get_terms(tree):\n",
    "        for leaf in leaves(tree):\n",
    "            term = [w for w,t in leaf if not w in stop_words]\n",
    "            yield term \n",
    "            \n",
    "    sentence_re = r'(?:(?:[A-Z])(?:.[A-Z])+.?)|(?:\\w+(?:-\\w+)*)|(?:\\$?\\d+(?:.\\d+)?%?)|(?:...|)(?:[][.,;\"\\'?():-_`])'\n",
    "    grammar = r\"\"\"\n",
    "    NBAR:\n",
    "        {<NN.*|JJ>*<NN.*>}  # Nouns and Adjectives, terminated with Nouns\n",
    "        \n",
    "    NP:\n",
    "        {<NBAR>}\n",
    "        {<NBAR><IN><NBAR>}  # Above, connected with in/of/etc...\n",
    "    \"\"\"\n",
    "    chunker = nltk.RegexpParser(grammar)\n",
    "    tokens = nltk.regexp_tokenize(x,sentence_re)\n",
    "    postoks = nltk.tag.pos_tag(tokens) #Part of speech tagging \n",
    "    tree = chunker.parse(postoks) #chunking\n",
    "    terms = get_terms(tree)\n",
    "    temp_phrases = []\n",
    "    for term in terms:\n",
    "        if len(term):\n",
    "            temp_phrases.append(' '.join(term))\n",
    "    \n",
    "    finalPhrase = [w for w in temp_phrases if w] #remove empty lists\n",
    "    return finalPhrase\n",
    "\n",
    "extractphraseRDD = joinedTokens.map(extractPhrase)\n",
    "extractphraseRDD.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FWlPvIT_w4KE"
   },
   "source": [
    "### Let's do a quick analysis of what are the most common phrases.\n",
    "### Could have also done this on individual words instead by skipping the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0wtS6oESw4KF",
    "outputId": "5fd5a50c-3045-43c9-c38f-4f031943f448"
   },
   "outputs": [],
   "source": [
    "freqDistRDD = extractphraseRDD.flatMap(lambda x : nltk.FreqDist(x).most_common()).map(lambda x: x).reduceByKey(lambda x,y : x+y).sortBy(lambda x: x[1], ascending = False)\n",
    "print(freqDistRDD.take(20))\n",
    "\n",
    "print('top ten words')\n",
    "print(lemwordsRDD.flatMap(lambda x : nltk.FreqDist(x).most_common()).map(lambda x: x).reduceByKey(lambda x,y : x+y).sortBy(lambda x: x[1], ascending = False).take(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yg4-v2sNw4KJ"
   },
   "source": [
    "### Let's take the phrase counts and turn them into visualizations by bringing these small sets of calculated results to pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "opFJpiu6w4KK",
    "outputId": "2993d7de-544e-45b1-b579-01e58d9bfc94"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "freqDistDF = freqDistRDD.toDF() #converting RDD to spark dataframe\n",
    "freqDistDF.createOrReplaceTempView(\"myTable\") \n",
    "df2 = spark.sql(\"SELECT _1 AS Keywords, _2 as Frequency from myTable limit 20\")\n",
    "pandD = df2.toPandas()\n",
    "pandD.plot.barh(x='Keywords', y='Frequency', rot=1, figsize=(10,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pXqUAvt1w4KN"
   },
   "source": [
    "### Even better, make it into a word cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lDGJrZGdw4KO"
   },
   "outputs": [],
   "source": [
    "#! pip3 install WordCloud\n",
    "%matplotlib inline \n",
    "from wordcloud import WordCloud\n",
    "wordcloudConvertDF = pandD.set_index('Keywords').T.to_dict('records')\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=100, relative_scaling=0.5, colormap='Dark2').generate_from_frequencies(dict(*wordcloudConvertDF))\n",
    "plt.figure(figsize=(14, 10))    \n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fsF1pSWCw4KS"
   },
   "source": [
    "### Word sentiment will attempt to automatically classify the content into Positive, Negative, or Neutral using basic understanding of word meanings in context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D7uSeRgAw4KT"
   },
   "outputs": [],
   "source": [
    "# Download this once and not inside a loop\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "def wordSentiment(x):\n",
    "    from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "    analyzer = SentimentIntensityAnalyzer() \n",
    "    senti_list_temp = []\n",
    "    for i in x:\n",
    "        y = ''.join(i) \n",
    "        vs = analyzer.polarity_scores(y)\n",
    "        senti_list_temp.append((y, vs))\n",
    "        senti_list_temp = [w for w in senti_list_temp if w]\n",
    "    sentiment_list  = []\n",
    "    for j in senti_list_temp:\n",
    "        first = j[0]\n",
    "        second = j[1]\n",
    "    \n",
    "        for (k,v) in second.items():\n",
    "            if k == 'compound':\n",
    "                if v < 0.0:\n",
    "                    sentiment_list.append((first, \"Negative\"))\n",
    "                elif v == 0.0:\n",
    "                    sentiment_list.append((first, \"Neutral\"))\n",
    "                else:\n",
    "                    sentiment_list.append((first, \"Positive\"))\n",
    "    return sentiment_list\n",
    "\n",
    "print(extractphraseRDD.take(1))\n",
    "sentimentRDD = extractphraseRDD.map(wordSentiment)\n",
    "print(sentimentRDD.take(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TNYD0iccw4KW"
   },
   "source": [
    "### The wordsentiment function we wrote takes a list of phrases and returns a list of tuples of phrases and sentiments. We could write another function to take a single string, but it's just easier to map the string to a single element list and back again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nQP2r_rdw4KX"
   },
   "outputs": [],
   "source": [
    "print(joinedTokens.take(1))\n",
    "sentencesentimentRDD = joinedTokens.map(lambda x : [x]).map(wordSentiment)\n",
    "print(sentencesentimentRDD.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T2mclZ95w4Ka"
   },
   "outputs": [],
   "source": [
    "#sentencesentimentRDD.filter(lambda x : x[0][1] == 'Negative').take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rBLWvSZuw4Kd"
   },
   "source": [
    "### Do an old school reduceByKey to see how many items we have of each sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uxlaFf4hw4Ke"
   },
   "outputs": [],
   "source": [
    "def sentimentCount(x):\n",
    "    return x.flatMap(lambda x : x).map(lambda x : (x[1], 1)).reduceByKey(lambda x, y : x + y)\n",
    "\n",
    "print('phrase count')\n",
    "print(sentimentCount(sentimentRDD).collect())\n",
    "print('review count')\n",
    "print(sentimentCount(sentencesentimentRDD).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XF3UB77Nw4Km"
   },
   "source": [
    "### Wrap it all up in a convenient helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xFFK5I9ew4Kn",
    "outputId": "7547b1a6-5bd7-4492-dfc9-30ddc2e8d9b3"
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def processText(df, language = 'english', additionalWords = {'u', 'r', 'im', 'ive'}):\n",
    "    import nltk\n",
    "    import string\n",
    "\n",
    "    def removePunctuations(x):\n",
    "        list_punct = list(string.punctuation)\n",
    "        filtered = [''.join(c for c in s if c not in list_punct) for s in x] \n",
    "        filtered_space = [s for s in filtered if s] #remove empty space \n",
    "        return filtered\n",
    "\n",
    "    def removeStopWords(x, language = 'english', additional_words = {}):\n",
    "        from nltk.corpus import stopwords\n",
    "        from nltk.corpus import stopwords\n",
    "        stop_words=set(stopwords.words(language)).union(additional_words)\n",
    "        filteredSentence = [w for w in x if not w in stop_words]\n",
    "        return filteredSentence\n",
    "\n",
    "    def lemmatize(x):\n",
    "        from nltk.stem import WordNetLemmatizer\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        finalLem = [lemmatizer.lemmatize(s) for s in x]\n",
    "        return finalLem\n",
    "\n",
    "    def joinTokensFunct(x):\n",
    "        joinedTokens_list = []\n",
    "        x = \" \".join(x)\n",
    "        return x\n",
    "\n",
    "    def extractPhrase(x):\n",
    "        from nltk.corpus import stopwords\n",
    "        stop_words=set(stopwords.words('english'))\n",
    "        def leaves(tree):\n",
    "            \"\"\"Finds NP (nounphrase) leaf nodes of a chunk tree.\"\"\"\n",
    "            for subtree in tree.subtrees(filter = lambda t: t.label()=='NP'):\n",
    "                yield subtree.leaves()\n",
    "\n",
    "        def get_terms(tree):\n",
    "            for leaf in leaves(tree):\n",
    "                term = [w for w,t in leaf if not w in stop_words]\n",
    "                yield term \n",
    "\n",
    "        sentence_re = r'(?:(?:[A-Z])(?:.[A-Z])+.?)|(?:\\w+(?:-\\w+)*)|(?:\\$?\\d+(?:.\\d+)?%?)|(?:...|)(?:[][.,;\"\\'?():-_`])'\n",
    "        grammar = r\"\"\"\n",
    "        NBAR:\n",
    "            {<NN.*|JJ>*<NN.*>}  # Nouns and Adjectives, terminated with Nouns\n",
    "\n",
    "        NP:\n",
    "            {<NBAR>}\n",
    "            {<NBAR><IN><NBAR>}  # Above, connected with in/of/etc...\n",
    "        \"\"\"\n",
    "        chunker = nltk.RegexpParser(grammar)\n",
    "        tokens = nltk.regexp_tokenize(x,sentence_re)\n",
    "        postoks = nltk.tag.pos_tag(tokens) #Part of speech tagging \n",
    "        tree = chunker.parse(postoks) #chunking\n",
    "        terms = get_terms(tree)\n",
    "        temp_phrases = []\n",
    "        for term in terms:\n",
    "            if len(term):\n",
    "                temp_phrases.append(' '.join(term))\n",
    "\n",
    "        finalPhrase = [w for w in temp_phrases if w] #remove empty lists\n",
    "        return finalPhrase\n",
    "\n",
    "    rdd = df.rdd.flatMap(lambda x: x)\n",
    "    sentence = rdd.map(lambda x : nltk.sent_tokenize(x))\n",
    "    word = sentence.map(lambda x : [word for line in x for word in line.split()])\n",
    "    nopunct = word.map(removePunctuations)\n",
    "    stopwords = nopunct.map(lambda x : removeStopWords(x, language, additionalWords))\n",
    "    lemwords = stopwords.map(lemmatize)\n",
    "    joinedTokens = lemwords.map(joinTokensFunct)\n",
    "    extractphrase = joinedTokens.map(extractPhrase)\n",
    "    return extractphrase\n",
    "\n",
    "def frequencyDistribution(x, plot = False):\n",
    "    df = x.flatMap(lambda x : nltk.FreqDist(x).most_common()).map(lambda x: x).reduceByKey(lambda x,y : x+y).sortBy(lambda x: x[1], ascending = False).toDF()\n",
    "    if plot:\n",
    "        df.createOrReplaceTempView(\"myTable\") \n",
    "        df2 = spark.sql(\"SELECT _1 AS Keywords, _2 as Frequency from myTable limit 20\")\n",
    "        pandD = df2.toPandas()\n",
    "        pandD.plot.barh(x='Keywords', y='Frequency', rot=1, figsize=(10,8))\n",
    "    return df\n",
    "\n",
    "def wordCloud(x):\n",
    "    from wordcloud import WordCloud\n",
    "    x.createOrReplaceTempView(\"myTable\") \n",
    "    df2 = spark.sql(\"SELECT _1 AS Keywords, _2 as Frequency from myTable limit 20\")\n",
    "    pandD = df2.toPandas()\n",
    "    wordcloudConvertDF = pandD.set_index('Keywords').T.to_dict('records')\n",
    "    wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=100, relative_scaling=0.5, colormap='Dark2').generate_from_frequencies(dict(*wordcloudConvertDF))\n",
    "    plt.figure(figsize=(14, 10))    \n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AkAQTG_Sw4Ku"
   },
   "source": [
    "### Using the helper functions, we can just select and fix the reviews from the original dataframe using spark sql in one line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O3JBWoUKw4Kv",
    "outputId": "135001d7-dc65-482c-d125-84672441dd5d"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "x = processText(spark.sql(\"select lower(verified_reviews) from alexa where verified_reviews is not null and verified_reviews <> 'verified_reviews'\"))\n",
    "freq = frequencyDistribution(x, False)\n",
    "print(freq.take(20))\n",
    "\n",
    "wordCloud(freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "byj-i9Dvw4Ky"
   },
   "source": [
    "### Using the results of the helper function, make a dataframe of the reviews and sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9-aW-OZpw4Kz"
   },
   "outputs": [],
   "source": [
    "sentiment2RDD = joinedTokens.map(lambda x : [x]).map(wordSentiment).map(lambda x : x[0])\n",
    "\n",
    "print(sentiment2RDD.take(10))\n",
    "sentimentDF = spark.createDataFrame(sentiment2RDD, schema='review:string, sentiment:string')\n",
    "display(sentimentDF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gHfNHJCMw4K2"
   },
   "source": [
    "### Use a pipeline to change the sentiment word into an index and tokenize, and convert the review text into an ML shaped dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XjikdsPlw4K3"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StringIndexer, OneHotEncoderEstimator\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "\n",
    "stages = [ StringIndexer(inputCol = 'sentiment', outputCol = 'label')\n",
    "         , Tokenizer(inputCol=\"review\", outputCol=\"words\")\n",
    "         , HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
    "         , IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "         ]\n",
    "          \n",
    "\n",
    "pipeline = Pipeline(stages = stages)\n",
    "dfMLFitted = pipeline.fit(sentimentDF)\n",
    "dfML = dfMLFitted.transform(sentimentDF)\n",
    "#display(dfML) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Au_nGFV2w4K6"
   },
   "source": [
    "### Let's explore the monitoring page by going to localhost:4040"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tpy-GTKIw4K7"
   },
   "source": [
    "### There's a lot of steps involved so since we may want to use this ML set many times, we can avoid recalculating the same transformations multiple times by persisting a copy of the dataframe in memory for the duration of this spark session or until we decide to unpersist it.\n",
    "\n",
    "#### There are many options for persisting to memory or disk or both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GXFnBx1jw4K7"
   },
   "source": [
    "p = pyspark.StorageLevel(useDisk = True, useMemory = True, useOffHeap = False, deserialized = True, replication = 1)\n",
    "\n",
    "DISK_ONLY\n",
    "StorageLevel(True, False, False, False, 1)\n",
    "\n",
    "DISK_ONLY_2\n",
    "StorageLevel(True, False, False, False, 2)\n",
    "\n",
    "MEMORY_AND_DISK\n",
    "StorageLevel(True, True, False, False, 1)\n",
    "\n",
    "MEMORY_AND_DISK_2\n",
    "StorageLevel(True, True, False, False, 2)\n",
    "\n",
    "MEMORY_AND_DISK_SER\n",
    "StorageLevel(True, True, False, False, 1)\n",
    "\n",
    "MEMORY_AND_DISK_SER_\n",
    "StorageLevel(True, True, False, False, 2)\n",
    "\n",
    "MEMORY_ONLY\n",
    "StorageLevel(False, True, False, False, 1)\n",
    "\n",
    "MEMORY_ONLY_2\n",
    "StorageLevel(False, True, False, False, 2)\n",
    "\n",
    "MEMORY_ONLY_SER\n",
    "StorageLevel(False, True, False, False, 1)\n",
    "\n",
    "MEMORY_ONLY_SER_2\n",
    "StorageLevel(False, True, False, False, 2)\n",
    "\n",
    "OFF_HEAP\n",
    "StorageLevel(True, True, True, False, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eSBsaZs3w4K8"
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "p = pyspark.StorageLevel(useDisk = True, useMemory = True, useOffHeap = False, deserialized = True, replication = 1)\n",
    "pyspark.StorageLevel(True, True, True, False, 1)\n",
    "#dfML.persist(pyspark.StorageLevel.MEMORY_AND_DISK)\n",
    "dfML.persist(p)\n",
    "display(dfML)\n",
    "train, test = dfML.randomSplit([.7,.3], seed = 100)\n",
    "train.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "glCjAjmjw4K_"
   },
   "source": [
    "### Switch to the browser and take a look now at the storage page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XkvTNBl-w4K_"
   },
   "source": [
    "### Let's just do a simple DecisionTreeClassifier on the ML dataset to predict which of the three labeled sentiments each review is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t6SczbK5w4LA"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'label', maxDepth = 6)\n",
    "%time dtModel = dt.fit(train)\n",
    "%time dtPredictions, dtLog = pyh.predict_and_evaluate(dtModel, test)\n",
    "print(dtLog)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "30fOHGl8w4LD"
   },
   "source": [
    "### Take a look at a sample of each prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5B-074fxw4LE"
   },
   "outputs": [],
   "source": [
    "print(dtPredictions.where('prediction = 0.0').select('review').take(1))\n",
    "print(dtPredictions.where(\"prediction = 1.0 and review<>''\").select('review').take(1))\n",
    "print(dtPredictions.where('prediction = 2.0').select('review').take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XpkLOmN6w4LJ"
   },
   "source": [
    "### We could save the trained model and then make a prediction one at a time using a helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EWk3UzfYw4LK"
   },
   "outputs": [],
   "source": [
    "def predictSentiment(trainedModel, transformModel, x):\n",
    "    newReview = sc.parallelize([(x,)])\n",
    "    #print('RDD', newReview.collect())\n",
    "    newReviewDF = spark.createDataFrame(newReview, schema='review:string')\n",
    "    #print('DF', newReviewDF.collect())\n",
    "    newReviewML = transformModel.transform(newReviewDF)\n",
    "    #print('ML',newReviewML.collect())\n",
    "    newPrediction = trainedModel.transform(newReviewML.select('features'))\n",
    "    return newPrediction\n",
    "\n",
    "prediction = predictSentiment(dtModel, dfMLFitted, 'I really love love love love my alexa' )\n",
    "print (prediction.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BnCJgRrSw4LQ"
   },
   "outputs": [],
   "source": [
    "for review in ['I really love love love love my alexa', 'sent 85 year old dad talk constantly', 'device interact home filled apple device disappointing']:\n",
    "    print(predictSentiment(dtModel, dfMLFitted, review).select('prediction').collect())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AXGbXGNJw4LU"
   },
   "source": [
    "### The cached dataframe will disappear automatically when a session ends, but you can unpersist it whenever you want. Run the following and flip back to the Storage tab in the browser and see that it is gone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GbUbYbfuw4LW"
   },
   "outputs": [],
   "source": [
    "dfML.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Xt2KFsiw4LZ"
   },
   "source": [
    "### Create a simple RDD to demonstrate accumlators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T0Z954aVw4LZ",
    "outputId": "2b8b48a0-be62-401b-b611-1de2a7dd79f1"
   },
   "outputs": [],
   "source": [
    "x0 = sc.parallelize(range(10))\n",
    "x1 = x0.map(lambda x : x * 2)\n",
    "print (x1.collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eYruqkrFw4Lc"
   },
   "source": [
    "### This won't work, so we need another way to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sinuWr2kw4Ld",
    "outputId": "fea4fa69-b3d0-451f-b2a8-cdd4a8500a5c"
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "def fun1(x):\n",
    "    global counter\n",
    "    counter += x\n",
    "    \n",
    "x0.foreach(fun1)\n",
    "print (counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CImLnoA4w4Lg"
   },
   "source": [
    "### Use an accumulator to create a global variable shared by all the workers for the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C5mJJfiHw4Lg",
    "outputId": "ca414e6a-8a29-4d3f-fccb-40014254c631"
   },
   "outputs": [],
   "source": [
    "counter = sc.accumulator(0)\n",
    "def fun2(x):\n",
    "    global counter\n",
    "    counter += x\n",
    "    \n",
    "x0.foreach(fun2)\n",
    "print (counter.value)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vnZ8lyaew4Lj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Ch10_NaturalLanguage.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
