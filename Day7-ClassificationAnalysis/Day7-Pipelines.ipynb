{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lU1Y1AWbQ7D4",
    "outputId": "0fb7bfbe-d5f2-4506-c4ce-71c186dc2d22"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "rootpath = '/class'\n",
    "datapath = f'{rootpath}/datasets/'\n",
    "sys.path.append(rootpath)\n",
    "import pyspark_helpers as pyh\n",
    "from pyspark_helpers import *\n",
    "sc, spark, conf = initspark()\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib as mp\n",
    "import numpy\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from pyspark_helpers import display\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pkhdr85WQ7EH"
   },
   "source": [
    "### The following helper function shows the building of stages to convert categorical and numeric columns into Vectorized versions using a Pipeline instead of building the steps as a series of DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NpKa0_m3Q7EJ"
   },
   "outputs": [],
   "source": [
    "def MakeMLPipeline(df, categorical_features, numeric_features, target_label = None, target_is_categorical = True):\n",
    "    from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler, StringIndexerModel\n",
    "    from pyspark.ml import Pipeline\n",
    "\n",
    "    stages = []\n",
    "\n",
    "    for c in categorical_features:\n",
    "        stringIndexer = StringIndexer(inputCol = c, outputCol = c + '_Index')\n",
    "        encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[c + \"_classVec\"])\n",
    "        stages += [stringIndexer, encoder]\n",
    "        \n",
    "    if target_is_categorical:\n",
    "        label_stringIdx = StringIndexer(inputCol = target_label, outputCol = 'label')\n",
    "        stages += [label_stringIdx]\n",
    "\n",
    "    assemblerInputs = [c + \"_classVec\" for c in categorical_features] + numeric_features\n",
    "    assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "    stages += [assembler]\n",
    "\n",
    "    pipeline = Pipeline(stages = stages)\n",
    "\n",
    "    return pipeline\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fFW7Fs8UQ7EQ"
   },
   "source": [
    "### Read the bank dataset ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l9EUwz1uQ7ET",
    "outputId": "057498b2-7e34-40d2-ca9e-2ce2807eb1fc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename = 'bank.csv'\n",
    "df = spark.read.csv(f'{datapath}/finance/{filename}', header = True, inferSchema = True)\n",
    "display(df)\n",
    "\n",
    "# Save a pointer to the raw data\n",
    "dfRawFile = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ARRZ1lzQ7Ea",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "start = timer()\n",
    "\n",
    "col = 'marital'\n",
    "m_indexer = StringIndexer(inputCol = col, outputCol = col+'_Index')\n",
    "x1 = m_indexer.fit(df).transform(df) #.select(col, col+'_Index')\n",
    "\n",
    "m_encoder = OneHotEncoderEstimator(inputCols=[col + '_Index'], outputCols=[col+'_Vector'])\n",
    "x2 = m_encoder.fit(x1).transform(x1).orderBy(col + '_Index')\n",
    "\n",
    "col = 'job'\n",
    "j_indexer = StringIndexer(inputCol = col, outputCol = col+'_Index')\n",
    "x3 = j_indexer.fit(x2).transform(x2)\n",
    "j_encoder = OneHotEncoderEstimator(inputCols=[col + '_Index'], outputCols=[col+'_Vector'])\n",
    "x4 = j_encoder.fit(x3).transform(x3)\n",
    "\n",
    "#display(x2.select('marital', 'marital_Index', 'marital_Vector'))\n",
    "end = timer()\n",
    "print('time to run', end - start)\n",
    "display(x2)\n",
    "\n",
    "start = timer()\n",
    "col = 'marital'\n",
    "m_indexer = StringIndexer(inputCol = col, outputCol = col+'_Index')\n",
    "m_encoder = OneHotEncoderEstimator(inputCols=[col + '_Index'], outputCols=[col+'_Vector'])\n",
    "#pipeline = Pipeline(stages = [m_indexer, m_encoder])\n",
    "\n",
    "col = 'job'\n",
    "j_indexer = StringIndexer(inputCol = col, outputCol = col+'_Index')\n",
    "j_encoder = OneHotEncoderEstimator(inputCols=[col + '_Index'], outputCols=[col+'_Vector'])\n",
    "\n",
    "v_encoder = VectorAssembler(inputCols = ['age','marital_Vector', 'job_Vector'], outputCol = 'features')\n",
    "pipeline = Pipeline(stages = [m_indexer, j_indexer, m_encoder, j_encoder, v_encoder])\n",
    "dfModel = pipeline.fit(df)\n",
    "#dfModel.save()\n",
    "dfML = dfModel.transform(df)\n",
    "end = timer()\n",
    "print('time to run', end - start)\n",
    "display(dfML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wzTSPPJ3Q7Ef"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "col = 'marital'\n",
    "indexer = StringIndexer(inputCol = col, outputCol = col+'_Index')\n",
    "x1 = indexer.fit(df).transform(df) #.select(col, col+'_Index')\n",
    "\n",
    "encoder = OneHotEncoderEstimator(inputCols=[col + '_Index'], outputCols=[col+'_Vector'])\n",
    "x2 = encoder.fit(x1).transform(x1).orderBy(col + '_Index')\n",
    "display(x2.select('marital', 'marital_Index', 'marital_Vector'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8GBDousIQ7El"
   },
   "source": [
    "### Use the same categorical and numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l-ZomoupQ7Eo"
   },
   "outputs": [],
   "source": [
    "# Let's just keep a few fields to start with for simplicity\n",
    "numeric_features = ['age','balance', 'duration', 'pdays']\n",
    "categorical_features = ['job', 'marital', 'education', 'housing', 'loan', 'contact', 'campaign', 'poutcome', 'deposit']\n",
    "\n",
    "# numeric_features = ['balance', 'duration', 'age']\n",
    "# categorical_features = ['marital', 'education']\n",
    "target_label = 'default'\n",
    "\n",
    "\n",
    "df = dfRawFile.select(numeric_features + categorical_features + [target_label])\n",
    "display(df)\n",
    "print(df.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MW2_THZsQ7Eu"
   },
   "source": [
    "### Try this using the original helper vs. the Pipeline version to see if there is a time difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Stwhv7nCQ7Ex"
   },
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "start = timer()\n",
    "\n",
    "dfModel, dfML = MakeMLDataFramePipeline(df, categorical_features, numeric_features, target_label)\n",
    "#dfML = pyh.MakeMLDataFramePipeline(df, categorical_features, numeric_features, target_label)\n",
    "#dfML = pyh.MakeMLDataFrame(df, categorical_features, numeric_features, target_label)\n",
    "\n",
    "display(dfML)\n",
    "dfML.printSchema()\n",
    "labelCnt = dfML.groupBy('label').count()\n",
    "display(labelCnt)\n",
    "\n",
    "end = timer()\n",
    "print('time to run', end - start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hNeM9WLSQ7E_"
   },
   "source": [
    "### Train and test as normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2FXs9zsbQ7FB"
   },
   "outputs": [],
   "source": [
    "train, test = dfML.randomSplit([.7,.3], seed = 10)\n",
    "print (f'Training set row count {train.count()}')\n",
    "print (f'Testing set row count {test.count()}')\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5qqXvpPhQ7FS"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'label', maxDepth = 3)\n",
    "dtModel = dt.fit(train)\n",
    "print('DT Trained')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KDrBdcgSQ7Fd"
   },
   "outputs": [],
   "source": [
    "dtPredictions, dtLog = pyh.predict_and_evaluate(dtModel, test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H1wRv-VKQ7Fn",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predict = dict(age=59, balance=2343, duration=1042, pdays=-1, job='admin.', marital='married', education='secondary', housing='yes', loan='no', contact='unknown', campaign=1, poutcome='unknown', deposit='yes')\n",
    "print(predict)\n",
    "predict = spark.createDataFrame(sc.parallelize([predict]))\n",
    "print(predict)\n",
    "predictML = dfModel.transform(predict)\n",
    "#x = dtModel.transform(predict)\n",
    "\n",
    "print(predictML.take(1))\n",
    "\n",
    "prediction = dtModel.transform(predictML).select('prediction')\n",
    "print(prediction.collect()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8TRuApNMQ7Fu"
   },
   "outputs": [],
   "source": [
    "def predict_bankdefault(transformModel, predictionModel, d): #age, balance, duration, pdays, job, marital, education, housing, loan, contact, campaign, poutcome, deposit):\n",
    "    newDF = spark.createDataFrame(sc.parallelize([d]))\n",
    "    predictML = transformModel.transform(newDF)\n",
    "    prediction = predictionModel.transform(predictML)\n",
    "    return (prediction.collect())[0][0]\n",
    "\n",
    "predict = dict(age=19, balance=2343, duration=1042, pdays=-1, job='admin.', marital='married', education='secondary', housing='yes', loan='no', contact='unknown', campaign=1, poutcome='unknown', deposit='yes')\n",
    "print (predict_bankdefault(dfModel, dtModel, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qwhj2P_QQ7Fz"
   },
   "source": [
    "### Pipelines and writing your own models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J4ISf2NoQ7F3"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.pipeline import Estimator, Model, Pipeline, Transformer\n",
    "from pyspark.ml.param.shared import *\n",
    "from pyspark.sql.functions import avg, stddev_samp\n",
    "\n",
    "\n",
    "class myTransformer(Transformer):\n",
    "    pass\n",
    "\n",
    "class HasMean(Params):\n",
    "\n",
    "    mean = Param(Params._dummy(), \"mean\", \"mean\", \n",
    "        typeConverter=TypeConverters.toFloat)\n",
    "\n",
    "    def __init__(self):\n",
    "        super(HasMean, self).__init__()\n",
    "\n",
    "    def setMean(self, value):\n",
    "        return self._set(mean=value)\n",
    "\n",
    "    def getMean(self):\n",
    "        return self.getOrDefault(self.mean)\n",
    "    \n",
    "class HasStandardDeviation(Params):\n",
    "\n",
    "    stddev = Param(Params._dummy(), \"stddev\", \"stddev\", \n",
    "        typeConverter=TypeConverters.toFloat)\n",
    "\n",
    "    def __init__(self):\n",
    "        super(HasStandardDeviation, self).__init__()\n",
    "\n",
    "    def setStddev(self, value):\n",
    "        return self._set(stddev=value)\n",
    "\n",
    "    def getStddev(self):\n",
    "        return self.getOrDefault(self.stddev)\n",
    "\n",
    "class HasCenteredThreshold(Params):\n",
    "\n",
    "    centered_threshold = Param(Params._dummy(),\n",
    "            \"centered_threshold\", \"centered_threshold\",\n",
    "            typeConverter=TypeConverters.toFloat)\n",
    "\n",
    "    def __init__(self):\n",
    "        super(HasCenteredThreshold, self).__init__()\n",
    "\n",
    "    def setCenteredThreshold(self, value):\n",
    "        return self._set(centered_threshold=value)\n",
    "\n",
    "    def getCenteredThreshold(self):\n",
    "        return self.getOrDefault(self.centered_threshold)\n",
    "    \n",
    "class NormalDeviation(Estimator, HasInputCol, \n",
    "        HasPredictionCol, HasCenteredThreshold):\n",
    "\n",
    "    def _fit(self, dataset):\n",
    "        c = self.getInputCol()\n",
    "        mu, sigma = dataset.agg(avg(c), stddev_samp(c)).first()\n",
    "        return (NormalDeviationModel()\n",
    "            .setInputCol(c)\n",
    "            .setMean(mu)\n",
    "            .setStddev(sigma)\n",
    "            .setCenteredThreshold(self.getCenteredThreshold())\n",
    "            .setPredictionCol(self.getPredictionCol()))\n",
    "\n",
    "class NormalDeviationModel(Model, HasInputCol, HasPredictionCol,\n",
    "        HasMean, HasStandardDeviation, HasCenteredThreshold):\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        x = self.getInputCol()\n",
    "        y = self.getPredictionCol()\n",
    "        threshold = self.getCenteredThreshold()\n",
    "        mu = self.getMean()\n",
    "        sigma = self.getStddev()\n",
    "\n",
    "        return dataset.withColumn(y, (dataset[x] - mu) > threshold * sigma)\n",
    "\n",
    "df = sc.parallelize([(1, 2.0), (2, 3.0), (3, 0.0), (4, 99.0)]).toDF([\"id\", \"x\"])\n",
    "\n",
    "normal_deviation = NormalDeviation().setInputCol(\"x\").setCenteredThreshold(1.0)\n",
    "model  = Pipeline(stages=[normal_deviation]).fit(df)\n",
    "#print(normal_deviation.getMean())\n",
    "model.transform(df).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9zClZkv3Q7F-"
   },
   "source": [
    "### PandasUDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CdVMo_TeQ7GC"
   },
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(pd.DataFrame(pd.Series(range(11)), columns=[\"x\"]))\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Il6beyitQ7GI"
   },
   "outputs": [],
   "source": [
    "def dbl(x):\n",
    "    return x * 2\n",
    "\n",
    "nums1 = [1, 2, 3, 4]\n",
    "nums2 = []\n",
    "for n in nums1:\n",
    "    nums2.append(dbl(n)) \n",
    "\n",
    "print (nums1 * 2)\n",
    "\n",
    "import numpy as np\n",
    "nums3 = np.array([1, 2, 3, 4])\n",
    "nums4 = nums3 * 2\n",
    "\n",
    "print (nums3 * 2)\n",
    "\n",
    "\n",
    "print(nums3 - nums3.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7ibhsGkuQ7GR"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def func1(x):\n",
    "    return x + 1\n",
    "\n",
    "display(df.withColumn('func1', udf(func1, 'int')(df.x)))\n",
    "\n",
    "func1x = udf(func1, 'int')\n",
    "display(df.withColumn('func1x', func1x(df.x)))\n",
    "\n",
    "sqr = udf(lambda x : x * x , 'int') \n",
    "display(df.withColumn('x3', sqr(df.x)))\n",
    "\n",
    "@udf('int')\n",
    "def square(x):\n",
    "      return x * 2\n",
    "\n",
    "display(df.withColumn('x2', square(df.x)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "10_DxWlGQ7GW"
   },
   "outputs": [],
   "source": [
    "#! pip install pyarrow\n",
    "#import pyarrow\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "#from pyspark.sql.types import LongType\n",
    "\n",
    "@pandas_udf('double', PandasUDFType.SCALAR)\n",
    "def psquare(x):\n",
    "      return x * x\n",
    "   \n",
    "#pandas_square = pandas_udf(psquare, returnType=LongType())\n",
    "\n",
    "df.withColumn('x3', psquare(df.x)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AAbuLn5HQ7Gg"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Day7-Pipelines.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
