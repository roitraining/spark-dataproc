{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gVESJgGmrOnE"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "rootpath = '/class'\n",
    "datapath = f'{rootpath}/datasets/'\n",
    "sys.path.append(rootpath)\n",
    "import pyspark_helpers as pyh\n",
    "from pyspark_helpers import *\n",
    "sc, spark, conf = initspark()\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib as mp\n",
    "import numpy\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from pyspark_helpers import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "93pu_qAYYvLG"
   },
   "source": [
    "### Let's read in a bank dataset to try to predict if a potential borrower will default on their loan before lending to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xOtG6pXJrOnN"
   },
   "outputs": [],
   "source": [
    "filename = 'bank.csv'\n",
    "df = spark.read.csv(f'{datapath}/finance/{filename}', header = True, inferSchema = True)\n",
    "display(df)\n",
    "\n",
    "# Save a pointer to the raw data\n",
    "dfRawFile = df\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "27HGsZ6ArOnW"
   },
   "source": [
    "### Explore numeric features to see if there is any correlation between values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RMFO0oVOrOnY",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "numeric_features = ['balance', 'duration', 'age']\n",
    "categorical_features = ['marital', 'education']\n",
    "display(pyh.describe_numeric_features(df, numeric_features))\n",
    "pyh.scatter_matrix(df, numeric_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring categorical columns is also useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xw_RBw9xrOnf",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "df.groupBy('default').count().toPandas().plot(kind = 'bar')\n",
    "df.groupBy('job').count().toPandas().plot(kind='bar')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's make a pipeline that combines several stages into one. First we will StringIndexer, then OneHotEncoder, then Vector Assemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler, StringIndexerModel\n",
    "from pyspark.ml import Pipeline\n",
    "# help(StringIndexer)\n",
    "# help(OneHotEncoderEstimator)\n",
    "\n",
    "maritalStringIndexer = StringIndexer(inputCol = 'marital', outputCol = 'marital_index')\n",
    "jobStringIndexer = StringIndexer(inputCol = 'job', outputCol = 'job_index')\n",
    "maritalOneHotEncoder = OneHotEncoderEstimator(inputCols=['marital_index'], outputCols=['marital_vector'])\n",
    "jobOneHotEncoder = OneHotEncoderEstimator(inputCols=['job_index'], outputCols=['job_vector'])\n",
    "assembler = VectorAssembler(inputCols=['balance', 'duration', 'age', 'marital_vector', 'job_vector']\n",
    "                            , outputCol='features')\n",
    "pipeline = Pipeline(stages = [maritalStringIndexer, jobStringIndexer, maritalOneHotEncoder, jobOneHotEncoder, assembler])\n",
    "pipeline_trained = pipeline.fit(df)\n",
    "df2 = pipeline_trained.transform(df)\n",
    "display(df2.limit(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing a lot of OneHotEncoding is pretty common so a helper function makes this a lot easier. This function will take a list of categorical columns and OneHotEncode them and vector assemble them with the numerical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QFuBR7QtQAFg"
   },
   "outputs": [],
   "source": [
    "def MakeMLPipeline(df, categorical_features, numeric_features, target_label = None, target_is_categorical = True):\n",
    "    from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler, StringIndexerModel\n",
    "    from pyspark.ml import Pipeline\n",
    "\n",
    "    stages = []\n",
    "\n",
    "    for c in categorical_features:\n",
    "        stringIndexer = StringIndexer(inputCol = c, outputCol = c + '_index')\n",
    "        encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[c + '_vector'])\n",
    "        stages += [stringIndexer, encoder]\n",
    "        \n",
    "    if target_is_categorical:\n",
    "        label_stringIdx = StringIndexer(inputCol = target_label, outputCol = 'label')\n",
    "        stages += [label_stringIdx]\n",
    "\n",
    "    assemblerInputs = numeric_features + [c + '_vector' for c in categorical_features]\n",
    "    assembler = VectorAssembler(inputCols=assemblerInputs, outputCol='features')\n",
    "    stages += [assembler]\n",
    "\n",
    "    pipeline = Pipeline(stages = stages)\n",
    "\n",
    "    return pipeline\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8itVD0ctYvLM"
   },
   "source": [
    "### Clean up the dataset by identifying the numeric and categorical features and target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o-FZqXtsrOnR"
   },
   "outputs": [],
   "source": [
    "# Let's just keep a few fields to start with for simplicity\n",
    "\n",
    "numeric_features = ['balance', 'duration', 'age']\n",
    "categorical_features = ['marital', 'education']\n",
    "\n",
    "# numeric_features = ['age','balance', 'duration', 'pdays']\n",
    "# categorical_features = ['job', 'marital', 'education', 'housing', 'loan', 'contact', 'campaign', 'poutcome', 'deposit']\n",
    "\n",
    "target_label = 'default'\n",
    "\n",
    "df = dfRawFile.select(numeric_features + categorical_features + [target_label])\n",
    "pipeline = MakeMLPipeline(df, categorical_features, numeric_features, target_label, target_is_categorical = True) \n",
    "pipeline_trained = pipeline.fit(df)\n",
    "df = pipeline_trained.transform(df)\n",
    "display(df.limit(3))\n",
    "print(df.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You could even insert extra steps in the stages returned before fitting it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import SQLTransformer\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import QuantileDiscretizer\n",
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "df = dfRawFile.select(numeric_features + categorical_features + [target_label])\n",
    "pipeline = MakeMLPipeline(df, categorical_features, numeric_features, target_label, target_is_categorical = True) \n",
    "sql = \"SELECT *, balance/1000 as newbalance FROM __THIS__\"\n",
    "sqlTransformer = SQLTransformer(statement = sql)\n",
    "stages = pipeline.getStages()\n",
    "stages.insert(-1, sqlTransformer)\n",
    "\n",
    "quantile = QuantileDiscretizer(inputCol='age', outputCol='age_buckers', numBuckets = 3)\n",
    "stages.insert(-1, quantile)\n",
    "\n",
    "minMaxScaler = MinMaxScaler(inputCol='features', outputCol='minmax_features')\n",
    "stages.append(minMaxScaler)\n",
    "\n",
    "standardScaler = StandardScaler(inputCol='features', outputCol='scaled_features')\n",
    "stages.append(standardScaler)\n",
    "\n",
    "pca = PCA(k=4, inputCol='features', outputCol='pcaFeatures')\n",
    "stages.append(pca)\n",
    "\n",
    "print(stages)\n",
    "\n",
    "\n",
    "pipeline_trained = pipeline.fit(df)\n",
    "df = pipeline_trained.transform(df)\n",
    "display(df.limit(3))\n",
    "print(df.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just go with this moving forward to classification\n",
    "\n",
    "numeric_features = ['age','balance', 'duration', 'pdays']\n",
    "categorical_features = ['job', 'marital', 'education', 'housing', 'loan', 'contact', 'campaign', 'poutcome', 'deposit']\n",
    "\n",
    "target_label = 'default'\n",
    "\n",
    "df = dfRawFile.select(numeric_features + categorical_features + [target_label])\n",
    "pipeline = MakeMLPipeline(df, categorical_features, numeric_features, target_label, target_is_categorical = True) \n",
    "pipeline_trained = pipeline.fit(df)\n",
    "pipeline_trained.write().overwrite().save('bank_pipeline')\n",
    "\n",
    "dfML = pipeline_trained.transform(df)\n",
    "display(dfML.limit(3))\n",
    "print(dfML.take(1))\n",
    "\n",
    "labelCnt = dfML.groupBy('label').count()\n",
    "display(labelCnt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LW8lIoT_QAGJ"
   },
   "source": [
    "### Once a trained pipeline model is saved, it can be reloaded to do the exact same transformation on new datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_fEZ2NYjQAGK",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "pipeline3 = PipelineModel.load('bank_pipeline')\n",
    "dfML3 = pipeline3.transform(df)\n",
    "display(dfML3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K3D1CPISYvLi"
   },
   "source": [
    "### Save the vectorized file in case we want to use it again. This saves the transformed data as opposed to the model that does the transformation in the first place. Both are worth saving for different reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4AevDiyFrOnp"
   },
   "outputs": [],
   "source": [
    "dfML.select('features', 'label').write.format('parquet').mode('overwrite').save('testsave')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WsfrorKKQAGc"
   },
   "source": [
    "### Load the saved file to see it worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MsJShfaQQAGd"
   },
   "outputs": [],
   "source": [
    "dfML0 = spark.read.format('parquet').load('testsave')\n",
    "dfML0.printSchema()\n",
    "display(dfML0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NJGH3eOQYvLn"
   },
   "source": [
    "### Split it into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FJ5WvSOErOnt"
   },
   "outputs": [],
   "source": [
    "#dfML = dfML0\n",
    "train, test = dfML.randomSplit([.7,.3], seed = 100)\n",
    "print (f'Training set row count {train.count()}')\n",
    "print (f'Testing set row count {test.count()}')\n",
    "display(train.groupBy('label').count())\n",
    "display(test.groupBy('label').count())\n",
    "#print(test.take(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x8jFDzYgYvLs"
   },
   "source": [
    "### Import the Decision Tree classifier and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wnOwIT8_rOnw"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'label', maxDepth = 6)\n",
    "dtModel = dt.fit(train)\n",
    "print('DT Trained')\n",
    "\n",
    "filename1 = filename.replace('.','_') + '_DT_trainedModel'\n",
    "dtModel.write().overwrite().save(filename1)\n",
    "print('DT Saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "14cTmWjgQAGx"
   },
   "source": [
    "### Now make predictions from the trained model and see how good of a job it did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kRqnFKDjYvLz"
   },
   "outputs": [],
   "source": [
    "predDT = dtModel.transform(test)\n",
    "#display(pred.limit(3))\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "print('Predicted')\n",
    "predDT.groupBy('prediction').count().show()\n",
    "print('Actual')\n",
    "predDT.groupBy('label').count().show()\n",
    "\n",
    "metrics = MulticlassMetrics(predDT.select(['prediction','label']).rdd) \n",
    "print(metrics.confusionMatrix().toArray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pHw_RkZfQAG5"
   },
   "source": [
    "### That scientific notation is annoying so try the following to make it more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kaPJfzaiQAG6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "print(metrics.confusionMatrix().toArray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This function just makes the output a bit easier to read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm, cmp = pyh.pretty_confusion(metrics.confusionMatrix().toArray(), include_percent = True)\n",
    "print(cm)\n",
    "print(cmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1r45uPTwQAG-"
   },
   "source": [
    "### You can also try reloading the saved model and seeing that it works. Note you have to import a different class name than before; this one ends with Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QIBpTxlDQAG_"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassificationModel\n",
    "dtModel2 = DecisionTreeClassificationModel.load(filename1)\n",
    "pred2 = dtModel2.transform(test)\n",
    "display(pred2.limit(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "msp5lm5MYvMF"
   },
   "source": [
    "### Now let's try Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "goWOCubRrOn5"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=10)\n",
    "lrModel = lr.fit(train)\n",
    "print('LR Trained')\n",
    "\n",
    "filename1 = filename.replace('.','_') + '_LR_trainedModel'\n",
    "lrModel.write().overwrite().save(filename1)\n",
    "print('LR Saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E6Tb65brYvMK"
   },
   "source": [
    "### Note again how to load a saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PVPv5t1irOn9"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "lrModel2 = LogisticRegressionModel.load(filename1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ukpqo10_YvMO"
   },
   "source": [
    "### See the test results as before, but LR has some extra options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RGB_IDUUrOoB"
   },
   "outputs": [],
   "source": [
    "predLR = lrModel.transform(test)\n",
    "display(predLR.limit(3))\n",
    "\n",
    "cm, cmp = pyh.pretty_confusion(predLR, include_percent = True)\n",
    "print(cm)\n",
    "print(cmp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PHP4twi0YvMV"
   },
   "source": [
    "### Let's try different thresholds to see if we can tweak the false positive/negative balance or improve the overall accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(10, 91, 10):\n",
    "    lr2 = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=10, threshold = t/100).fit(train)\n",
    "    predLR2 = lr2.transform(test)\n",
    "    cm, cmp = pyh.pretty_confusion(predLR2, include_percent = True)\n",
    "    print(f'Threshold {t}')\n",
    "    print(cm)\n",
    "    print(cmp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "87GA4AJ6YvMr"
   },
   "source": [
    "### After a while it's the same thing over and over, but try out as many models as possible to see which works best.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XoN-fiT2rOoE"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'label', numTrees = 10, maxDepth = 6)\n",
    "rfModel = rf.fit(train)\n",
    "print('RF Trained')\n",
    "\n",
    "filename1 = filename.replace('.','_') + '_RF_trainedModel'\n",
    "rfModel.write().overwrite().save(filename1)\n",
    "print('RF Saved')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aF0GSzKXrOoK"
   },
   "outputs": [],
   "source": [
    "predRF = rfModel.transform(test)\n",
    "cm, cmp = pyh.pretty_confusion(predRF, include_percent = True)\n",
    "print(cm)\n",
    "print(cmp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zAo9F-w5YvM2"
   },
   "source": [
    "### Try Gradient Boost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "90o6tiojrOoO"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "gbt = GBTClassifier(maxIter=10)\n",
    "gbtModel = gbt.fit(train)\n",
    "print ('GBT Trained')\n",
    "\n",
    "filename1 = filename.replace('.','_') + '_GBT_trainedModel'\n",
    "rfModel.write().overwrite().save(filename1)\n",
    "print ('GBT Saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wuna45UTrOoR"
   },
   "outputs": [],
   "source": [
    "predGB = rfModel.transform(test)\n",
    "cm, cmp = pyh.pretty_confusion(predGB, include_percent = True)\n",
    "print(cm)\n",
    "print(cmp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NvV-i7hmYvM-"
   },
   "source": [
    "### Try Neural Networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6QPLoCu1rOoV"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# specify layers for the neural network:\n",
    "# input layer of size 13 (features), two intermediate of size 5 and 4\n",
    "# and output of size 2 (classes)\n",
    "layers = [13, 5, 4, 2]\n",
    "\n",
    "nn = MultilayerPerceptronClassifier(maxIter=100, layers=layers, blockSize=128, seed=1234)\n",
    "nnModel = nn.fit(train)\n",
    "print ('NN Trained')\n",
    "\n",
    "filename1 = filename.replace('.','_') + '_NN_trainedModel'\n",
    "nnModel.write().overwrite().save(filename1)\n",
    "print ('NN Saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1YxOJsY1YvNC"
   },
   "outputs": [],
   "source": [
    "predNN = rfModel.transform(test)\n",
    "cm, cmp = pyh.pretty_confusion(predNN, include_percent = True)\n",
    "print(cm)\n",
    "print(cmp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's make a function that can do a single real time prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "45t0JpRhYvNN"
   },
   "outputs": [],
   "source": [
    "def predict_bankdefault(transformModel, predictionModel, d): \n",
    "    #age, balance, duration, pdays, job, marital, education, housing, loan, contact, campaign, poutcome, deposit):\n",
    "\n",
    "    newDF = spark.createDataFrame(sc.parallelize([d]))\n",
    "    predictML = transformModel.transform(newDF)\n",
    "    prediction = predictionModel.transform(predictML)\n",
    "    return (prediction.select('prediction').collect())[0][0]\n",
    "\n",
    "predict1 = dict(age=19, balance=2343, duration=1042, pdays=-1, job='admin.', marital='married', education='secondary', housing='yes', loan='no', contact='unknown', campaign=1, deposit='yes', poutcome = 'unknown')\n",
    "print(predict_bankdefault(pipeline3, dtModel, predict1))\n",
    "\n",
    "predict2 = dict(age=31, balance=-825, duration=179, pdays=-1, job='unemployed', marital='married', education='secondary', housing='yes', loan='no', contact='unknown', campaign=1, deposit='yes', poutcome='unknown')\n",
    "print(predict_bankdefault(pipeline3, dtModel, predict2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Ch06_ClassificationAnalysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
