{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-JdIfR70Jxu2"
   },
   "source": [
    "### Set up the Spark environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "result = subprocess.run([\"gcloud\", \"config\", \"get-value\", \"project\"], stdout=subprocess.PIPE)\n",
    "PROJECT_ID = result.stdout.decode(\"utf-8\").strip()\n",
    "\n",
    "# Print the output\n",
    "print(PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SOURCE_PATH = f'gs://{PROJECT_ID}-data/datasets'\n",
    "DEST_PATH = f'gs://{PROJECT_ID}-output'\n",
    "print(SOURCE_PATH, DEST_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6vl7-cmUJxu-"
   },
   "source": [
    "### Turn a simple RDD into a DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sou-X5kCJxu_",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = sc.parallelize([(1,'alpha'),(2,'beta')])\n",
    "x0 = spark.createDataFrame(x)\n",
    "x0.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E_1Cg_xuJxvE"
   },
   "source": [
    "### Give the DataFrame meaningful column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "udSoOBO5JxvF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "x1 = spark.createDataFrame(x, schema=['ID','Name'])\n",
    "x1.show()\n",
    "print(x1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uiGdbXBVJxvJ"
   },
   "source": [
    "### Give a DataFrame a schema with column names and data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "en8OdGGlJxvK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "x2 = spark.createDataFrame(x, 'ID:int, Name:string')\n",
    "x2.show()\n",
    "print(x2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ex6IIfYYJxxp"
   },
   "source": [
    "### Create a schema object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BEYmzqk8Jxxq",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n",
    "schema1 = StructType([\n",
    "    StructField('ID', IntegerType()), \n",
    "    StructField('Name', StringType())\n",
    "])\n",
    "x3 = spark.createDataFrame(x, schema = schema1)\n",
    "x3.show()\n",
    "print(x3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XT2o_wyuJxvi"
   },
   "source": [
    "### The built in toDF method does the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sdjwqSI6Jxvj",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "x.toDF().printSchema()\n",
    "x.toDF(['ID', 'Name']).printSchema()\n",
    "x.toDF('ID:int, Name:string').printSchema()\n",
    "x.toDF(schema = schema1).printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rdAF7yVmJxvy"
   },
   "source": [
    "## LAB: ## \n",
    "### Use the regions and territories RDDs from the previous lab and convert them into DataFrames with meaningful schemas.\n",
    "<br>\n",
    "<details><summary>Click for <b>hint</b></summary>\n",
    "<p>\n",
    "Use sc.textFile to read the files\n",
    "<br>\n",
    "Use map functions to split and convert the data\n",
    "<br>\n",
    "Use spark.createDataFrame and toDF to convert RDD into DataFrames\n",
    "<br>\n",
    "<br>\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "<details><summary>Click for <b>code</b></summary>\n",
    "<p>\n",
    "\n",
    "```python\n",
    "regions = sc.textFile(f'{SOURCE_PATH}/northwind/CSV/regions')\n",
    "regions = regions.map(lambda x : x.split(',')).map(lambda x : (int(x[0]), x[1]))\n",
    "regionsdf = spark.createDataFrame(regions, 'RegionID:int, RegionName:string')\n",
    "regionsdf.show()\n",
    "\n",
    "territories = sc.textFile(f'{SOURCE_PATH}/northwind/CSV/territories')\n",
    "territories = territories.map(lambda x : x.split(',')).map(lambda x : (int(x[0]), x[1], int(x[2])))\n",
    "territoriesdf = territories.toDF('TerritoryID:int, TerritoryName:string, RegionID: int')\n",
    "territoriesdf.show()\n",
    "```\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aIbiLKz0Jxvz",
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f5J3KIQfJxw5"
   },
   "source": [
    "### Examples of reading a CSV directly into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tJWVKyXeJxw6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename = f'{SOURCE_PATH}/northwind/CSV/categories'\n",
    "cat1 = spark.read.load(filename, format = 'csv', sep = ',', inferSchema = True, header = False)\n",
    "cat1.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hgxblgrarL18",
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat1 = spark.read.load(filename, format = 'csv', sep = ',', inferSchema = True, header = True)\n",
    "cat1.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_er2IqEMrL2A"
   },
   "source": [
    "### There are several alternate syntaxes which can be confusing, but since you will encounter them, you need to learn to recognize the different options\n",
    "option and options allow you pass parameters in different ways, but not the true is quoted and lowercase because it is a java value, but you could also pass it as a True Python value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GmIrAg1KJxxA",
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat2 = spark.read.format('csv').option('header','true').option('inferSchema','true').load(filename)\n",
    "cat2.printSchema()\n",
    "cat2 = spark.read.format('csv').options(header=True, inferSchema='true').load(filename)\n",
    "cat2.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dtgs8nowrL2F"
   },
   "source": [
    "### If there is a top level read function for the file type you want, that's the cleanest option, and pass in the parameters as named parameters. Not all formats have this and also legacy code written before this may use the old style syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JQVzZUyVJxxE",
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat3 = spark.read.csv(filename, header = True, inferSchema = True)\n",
    "cat3.printSchema()\n",
    "cat3.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lCU_UT9UrL2L"
   },
   "source": [
    "### As the tables get more complex, there is a Jupyter command that will show the tables in a prettier format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pDAgmHbOrL2M",
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bvxqsBmgrL2R"
   },
   "source": [
    "## LAB: ## \n",
    "### Load the products table using any of the spark.read methods.\n",
    "<br>\n",
    "<details><summary>Click for <b>hint</b></summary>\n",
    "<p>\n",
    "Use spark.read.csv\n",
    "<br>\n",
    "Make sure to read the version that has headers if you want to infer schema\n",
    "<br><br>\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "<details><summary>Click for <b>code</b></summary>\n",
    "<p>\n",
    "\n",
    "```python\n",
    "prod1 = spark.read.csv(f'{SOURCE_PATH}/northwind/CSVHeaders/products', header=True, inferSchema=True) \n",
    "prod1.printSchema() \n",
    "prod1.show()\n",
    "```\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "84HQkI0drL2S",
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y9nYFZdErL2W"
   },
   "source": [
    "### Using a schema is a good idea for performance if you know what it is. Usually you can infer schema during development and use it as a helper to build the schema to use for production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UzaLUc5frL2X",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "prodSchema = StructType([\n",
    "    StructField('ProductID', IntegerType()), \n",
    "    StructField('ProductName', StringType()),\n",
    "    StructField('SupplierID', IntegerType()), \n",
    "    StructField('CategoryID', IntegerType()), \n",
    "    StructField('QuantityPerUnit', StringType()), \n",
    "    StructField('UnitPrice', FloatType()), \n",
    "    StructField('UnitsInStock', IntegerType()), \n",
    "    StructField('UnitsOnOrder', IntegerType()), \n",
    "    StructField('ReorderLevel', IntegerType()), \n",
    "    StructField('Discontinued', IntegerType())\n",
    "])\n",
    "\n",
    "prod2 = spark.read.csv(f'{SOURCE_PATH}/northwind/CSVHeaders/products', header=True, schema=prodSchema)\n",
    "print(prod2)\n",
    "prod2.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CqpW0HPKJxv2"
   },
   "source": [
    "### Convert a DataFrame into a JSON string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I64RwvUdJxv3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print (cat2.toJSON().take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cn23EMQcrL2h"
   },
   "source": [
    "### JSON is another top level supported format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ypuDVM-crL2i",
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat4 = spark.read.json(f'{SOURCE_PATH}/northwind/JSON/categories')\n",
    "cat4.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "upJodEzYrL2p"
   },
   "source": [
    "### You can also use schemas but be careful of case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VUo_rYPIrL2t",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "prod = spark.read.json(f'{SOURCE_PATH}/northwind/JSON/products')\n",
    "prod.show()\n",
    "\n",
    "prodSchema = StructType([\n",
    "    StructField('productid', IntegerType()), \n",
    "    StructField('productname', StringType()),\n",
    "    StructField('supplierid', IntegerType()), \n",
    "    StructField('categoryid', IntegerType()), \n",
    "    StructField('quantityperunit', StringType()), \n",
    "    StructField('unitprice', FloatType()), \n",
    "    StructField('unitsinstock', IntegerType()), \n",
    "    StructField('unitsonorder', IntegerType()), \n",
    "    StructField('reorderlevel', IntegerType()), \n",
    "    StructField('discontinued', IntegerType())\n",
    "])\n",
    "prod = spark.read.json(f'{SOURCE_PATH}/northwind/JSON/products', schema=prodSchema)\n",
    "prod.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GJAzAmRjrL2z"
   },
   "source": [
    "### You may also see the older style syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d4Lh5zjLrL20",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prod = spark.read.format('json').load(f'{SOURCE_PATH}/northwind/JSON/products')\n",
    "prod.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a7TI07ZPrL3X"
   },
   "source": [
    "### We can easily save this to a Hive table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v0JDyefiJxv7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prod.write.mode('overwrite').saveAsTable('products')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It's also easy to load data from a saved Hive table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prod2 = spark.read.table('products')\n",
    "prod2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s2PVAJGSJxv-"
   },
   "source": [
    "### Choose particular columns from a DataFrame.\n",
    "You can use quoted strings for the column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "shp65QEQJxv_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prod.select('productid', 'productname', 'unitprice').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_TuZyScErL3i"
   },
   "source": [
    "### Or you can use a pythonic syntax using the DataFrame name and field name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U96JctQ0rL3i",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prod.select(prod.productid, prod.productname, prod.unitprice).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1D_4iv3WrL3l"
   },
   "source": [
    "### Case is ignored if you use quoted strings but not if you use python syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RFILXweIrL3m",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prod.select('Productid', 'productname', 'unitprice').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prod.select(prod.Productid, prod.productname, prod.unitprice).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BTcz7pkirL3p"
   },
   "source": [
    "### Distinct is a method after the select method chooses the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xOXLR2IPJxwD",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prod.select('CategoryID').distinct().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w9NySqdqJxwH"
   },
   "source": [
    "### Sort a DataFrame. The sort and orderBy methods are different aliases for the exact same method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i9IkEP-4JxwI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prod.sort(prod.unitprice).show()\n",
    "prod.orderBy('unitprice', ascending = False).show()\n",
    "prod.select('productid', 'productname', 'unitprice').orderBy('unitprice').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I6zM9-5MJxwS"
   },
   "source": [
    "### Create a new DataFrame with a new calculated column added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XfXZYr82JxwU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prod2 = prod.withColumn('value', prod.unitprice * prod.unitsinstock)\n",
    "prod2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xImpvfmVJxwb"
   },
   "source": [
    "### Remove an unwanted column from a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AvbveAjfJxwd",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "prod2 = prod2.drop('quantityperunit')\n",
    "prod2.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZsHhY9u9Jxwh"
   },
   "source": [
    "### The filter and where methods can both be used and have alternative ways to represent the condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-bg7uhoEJxwj",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "p = prod\n",
    "p.filter(p.unitprice > 100).show()\n",
    "p.filter('unitprice > 100').show()\n",
    "# Note == when using python syntax\n",
    "p.where(p.categoryid == 2).show()\n",
    "# Note = when using quoted SQL like syntax\n",
    "p.where('categoryid = 2').show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dUnUFFZXrL3-"
   },
   "source": [
    "### More complex conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A_MDNTlbrL4A",
    "tags": []
   },
   "outputs": [],
   "source": [
    "p.where('unitprice >= 50 and unitprice <= 100').show()\n",
    "\n",
    "p.where('unitprice between 50 and 100').show()\n",
    "\n",
    "p.where((p.unitprice >=50) & (p.unitprice <= 100)).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k5E4afFTJxwp"
   },
   "source": [
    "## LAB: ## \n",
    "### Find all the products in category 2 with fewer units in stock than units on order \n",
    "### Only display with productid, name, unitsinstock, unitsonorder and unitprice\n",
    "<br>\n",
    "<details><summary>Click for <b>hint</b></summary>\n",
    "<p>\n",
    "Use the where or filter method. It's probably easier to use a quoted SQL style syntax\n",
    "<br>\n",
    "Use select to get the columns you want to see\n",
    "<br><br>\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "<details><summary>Click for <b>code</b></summary>\n",
    "<p>\n",
    "\n",
    "```python\n",
    "p.where('unitsinstock < unitsonorder and categoryid = 2').select('productid','productname', 'unitsinstock', 'unitsonorder', 'unitprice').show()\n",
    "```\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6kZwJv9LJxwr",
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yRYHW5DlJxwx"
   },
   "source": [
    "### JOINs work as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wLGYJT1PJxwy",
    "tags": []
   },
   "outputs": [],
   "source": [
    "tab1 = sc.parallelize([(1, 'Alpha'), (2, 'Beta'), (3, 'Delta')]).toDF('ID:int, code:string')\n",
    "tab2 = sc.parallelize([(100, 'One', 1), (101, 'Two', 2), (102, 'Three', 1), (103, 'Four', 4)]).toDF('ID:int, name:string, parentID:int')\n",
    "tab1.join(tab2, tab1.ID == tab2.parentID).show()\n",
    "tab1.join(tab2, tab1.ID == tab2.parentID, 'left').show()\n",
    "tab1.join(tab2, tab1.ID == tab2.parentID, 'right').show()\n",
    "tab1.join(tab2, tab1.ID == tab2.parentID, 'full').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FbKSw8mFJxw1"
   },
   "source": [
    "###  Examples of aggregate functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Q-LPpNaJxw2",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "tab3 = sc.parallelize([(1, 10), (1, 20), (1, 30), (2, 40), (2,50)]).toDF('groupID:int, amount:int')\n",
    "tab3.groupby('groupID').max().show()\n",
    "tab3.groupby('groupID').sum().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iArjkg9ErL4b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = tab3.groupby('groupID')\n",
    "x.agg({'amount':'sum'}).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dsi0GmVyrL4e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "x.agg(F.sum('amount'), F.max('amount')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VB_yhwpjrL4j",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "x.agg(expr('sum(amount) as total')).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dDGFU2bwJxxN"
   },
   "source": [
    "## LAB: ## \n",
    "### Join products and categories together displaying only the product and category ID's and names, sort by categoryid and productid\n",
    "<br>\n",
    "<details><summary>Click for <b>hint</b></summary>\n",
    "<p>\n",
    "Make sure not to show the common column twice\n",
    "<br>\n",
    "Select with python style makes it easier to distinguish which columns you want from a join\n",
    "<br><br>\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "<details><summary>Click for <b>code</b></summary>\n",
    "<p>\n",
    "\n",
    "```python\n",
    "filename = f'{SOURCE_PATH}/northwind/CSVHeaders/categories'\n",
    "c = spark.read.load(filename, format = 'csv', sep = ',', inferSchema = True, header = True)\n",
    "p = spark.read.json(f'{SOURCE_PATH}/northwind/JSON/products')\n",
    "c.show()\n",
    "p.show()\n",
    "\n",
    "j = (c.join(p, c.CategoryID == p.categoryid)\n",
    "      .select(c.CategoryID, c.CategoryName, p.productid, p.productname)\n",
    "      .orderBy('CategoryID', 'productid')\n",
    "    )\n",
    "j.show()\n",
    "```\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cTxpSZd7JxxP",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2pxxBU4AJxxS"
   },
   "source": [
    "### Sometimes you want to just rename a column so here are two ways to accomplish that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0RUqQyieJxxU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "p.withColumnRenamed('unitprice','listprice').show()\n",
    "cols = p.columns # get a list of all the current column names\n",
    "cols[5] = 'listprice' # replace a column position with the new name \n",
    "p1 = p.toDF(*cols) # create a new dataframe from the original with a list of column names\n",
    "p1.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uRz6DsSUrL4w"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Day2-DataFrames.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
