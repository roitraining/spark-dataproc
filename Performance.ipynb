{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4ed9e96-6973-4a67-94ba-7c4070f11fde",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial number of partitions: 4\n",
      "Number of partitions after filtering: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after filtering: 3333011\n",
      "Target number of partitions: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:============================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions after repartitioning: 1\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[user_id#7], functions=[count(1)])\n",
      "   +- HashAggregate(keys=[user_id#7], functions=[partial_count(1)])\n",
      "      +- Exchange SinglePartition, REPARTITION_BY_NUM, [plan_id=133]\n",
      "         +- Project [user_id#7]\n",
      "            +- Filter (isnotnull(event_type#11) AND (event_type#11 = purchase))\n",
      "               +- Project [user_id#7, [login,logout,purchase][cast(FLOOR((rand(-3131851920966333594) * 3.0)) as int)] AS event_type#11]\n",
      "                  +- Project [cast((rand(-7079338601226393055) * 10000.0) as int) AS user_id#7]\n",
      "                     +- Range (0, 10000000, step=1, splits=4)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "max1 = max\n",
    "from pyspark.sql.functions import *\n",
    "max = max1\n",
    "\n",
    "# spark = SparkSession.builder.appName(\"RepartitionExample\").getOrCreate()\n",
    "\n",
    "# Create a large DataFrame (e.g., representing web server logs)\n",
    "num_rows = 10000000  # 10 million rows\n",
    "df = spark.range(num_rows).withColumn(\"event_time\", current_timestamp()) \\\n",
    "    .withColumn(\"user_id\", (rand() * 10000).cast(\"int\")) \\\n",
    "    .withColumn(\"event_type\", array(lit(\"login\"), lit(\"logout\"), lit(\"purchase\"))[floor(rand() * 3).cast(\"int\")])\n",
    "\n",
    "# Initial number of partitions\n",
    "initial_partitions = df.rdd.getNumPartitions()\n",
    "print(f\"Initial number of partitions: {initial_partitions}\")\n",
    "\n",
    "filtered_df = df.filter(col(\"event_type\") == \"purchase\")\n",
    "\n",
    "# Number of partitions AFTER filtering\n",
    "filtered_partitions = filtered_df.rdd.getNumPartitions()\n",
    "print(f\"Number of partitions after filtering: {filtered_partitions}\")\n",
    "\n",
    "# Check the number of rows after filtering\n",
    "filtered_count = filtered_df.count()\n",
    "print(f\"Number of rows after filtering: {filtered_count}\")\n",
    "\n",
    "# If the filtered data is significantly smaller but still has the same number of partitions, it's inefficient.\n",
    "# Let's say, after filtering, we only have 1/10th of the rows but the same number of partitions.\n",
    "\n",
    "# Determine an appropriate number of partitions for the filtered data.\n",
    "# A general guideline is to aim for partitions of around 100-200MB each.\n",
    "# For simplicity, let's just reduce the number of partitions proportionally to the reduction in data size.\n",
    "target_partitions = max(1, int(filtered_count / (num_rows / initial_partitions) / 10))\n",
    "print(f\"Target number of partitions: {target_partitions}\")\n",
    "\n",
    "# Repartition the filtered DataFrame\n",
    "repartitioned_df = filtered_df.repartition(target_partitions)\n",
    "\n",
    "# Number of partitions AFTER repartitioning\n",
    "repartitioned_partitions = repartitioned_df.rdd.getNumPartitions()\n",
    "print(f\"Number of partitions after repartitioning: {repartitioned_partitions}\")\n",
    "\n",
    "# Perform a subsequent operation on the repartitioned DataFrame (e.g., aggregation)\n",
    "# This operation will now be more efficient due to better resource utilization\n",
    "aggregated_df = repartitioned_df.groupBy(\"user_id\").count()\n",
    "aggregated_df.explain() #show the execution plan\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "668869b3-3aae-4a24-a594-6760d00dddc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/23 04:31:28 INFO SparkEnv: Registering MapOutputTracker\n",
      "25/01/23 04:31:28 INFO SparkEnv: Registering BlockManagerMaster\n",
      "25/01/23 04:31:28 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "25/01/23 04:31:28 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "25/01/23 04:31:28 WARN Client: Same path resource file:///root/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.5.0.jar added multiple times to distributed cache.\n",
      "25/01/23 04:31:28 WARN Client: Same path resource file:///root/.ivy2/jars/org.tukaani_xz-1.9.jar added multiple times to distributed cache.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without Caching:\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[customer#35], functions=[sum(sales#37L)])\n",
      "   +- Exchange hashpartitioning(customer#35, 1000), ENSURE_REQUIREMENTS, [plan_id=152]\n",
      "      +- HashAggregate(keys=[customer#35], functions=[partial_sum(sales#37L)])\n",
      "         +- Project [customer#35, sales#37L]\n",
      "            +- Scan ExistingRDD[customer#35,product#36,sales#37L]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|customer|total_sales|\n",
      "+--------+-----------+\n",
      "|       B|        630|\n",
      "|       C|        130|\n",
      "|       A|        460|\n",
      "+--------+-----------+\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[], functions=[avg(total_sales#45L)])\n",
      "   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=222]\n",
      "      +- HashAggregate(keys=[], functions=[partial_avg(total_sales#45L)])\n",
      "         +- HashAggregate(keys=[customer#35], functions=[sum(sales#37L)])\n",
      "            +- Exchange hashpartitioning(customer#35, 1000), ENSURE_REQUIREMENTS, [plan_id=218]\n",
      "               +- HashAggregate(keys=[customer#35], functions=[partial_sum(sales#37L)])\n",
      "                  +- Project [customer#35, sales#37L]\n",
      "                     +- Scan ExistingRDD[customer#35,product#36,sales#37L]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|    average_sales|\n",
      "+-----------------+\n",
      "|406.6666666666667|\n",
      "+-----------------+\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[], functions=[max(total_sales#45L)])\n",
      "   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=336]\n",
      "      +- HashAggregate(keys=[], functions=[partial_max(total_sales#45L)])\n",
      "         +- HashAggregate(keys=[customer#35], functions=[sum(sales#37L)])\n",
      "            +- Exchange hashpartitioning(customer#35, 1000), ENSURE_REQUIREMENTS, [plan_id=332]\n",
      "               +- HashAggregate(keys=[customer#35], functions=[partial_sum(sales#37L)])\n",
      "                  +- Project [customer#35, sales#37L]\n",
      "                     +- Scan ExistingRDD[customer#35,product#36,sales#37L]\n",
      "\n",
      "\n",
      "+---------+\n",
      "|max_sales|\n",
      "+---------+\n",
      "|      630|\n",
      "+---------+\n",
      "\n",
      "\n",
      "With Caching:\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- InMemoryTableScan [customer#35, total_sales#105L]\n",
      "      +- InMemoryRelation [customer#35, total_sales#105L], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "            +- AdaptiveSparkPlan isFinalPlan=false\n",
      "               +- HashAggregate(keys=[customer#35], functions=[sum(sales#37L)])\n",
      "                  +- Exchange hashpartitioning(customer#35, 1000), ENSURE_REQUIREMENTS, [plan_id=446]\n",
      "                     +- HashAggregate(keys=[customer#35], functions=[partial_sum(sales#37L)])\n",
      "                        +- Project [customer#35, sales#37L]\n",
      "                           +- Scan ExistingRDD[customer#35,product#36,sales#37L]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|customer|total_sales|\n",
      "+--------+-----------+\n",
      "|       B|        630|\n",
      "|       C|        130|\n",
      "|       A|        460|\n",
      "+--------+-----------+\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[], functions=[avg(total_sales#105L)])\n",
      "   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=496]\n",
      "      +- HashAggregate(keys=[], functions=[partial_avg(total_sales#105L)])\n",
      "         +- InMemoryTableScan [total_sales#105L]\n",
      "               +- InMemoryRelation [customer#35, total_sales#105L], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                     +- AdaptiveSparkPlan isFinalPlan=false\n",
      "                        +- HashAggregate(keys=[customer#35], functions=[sum(sales#37L)])\n",
      "                           +- Exchange hashpartitioning(customer#35, 1000), ENSURE_REQUIREMENTS, [plan_id=499]\n",
      "                              +- HashAggregate(keys=[customer#35], functions=[partial_sum(sales#37L)])\n",
      "                                 +- Project [customer#35, sales#37L]\n",
      "                                    +- Scan ExistingRDD[customer#35,product#36,sales#37L]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|    average_sales|\n",
      "+-----------------+\n",
      "|406.6666666666667|\n",
      "+-----------------+\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[], functions=[max(total_sales#105L)])\n",
      "   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=561]\n",
      "      +- HashAggregate(keys=[], functions=[partial_max(total_sales#105L)])\n",
      "         +- InMemoryTableScan [total_sales#105L]\n",
      "               +- InMemoryRelation [customer#35, total_sales#105L], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                     +- AdaptiveSparkPlan isFinalPlan=false\n",
      "                        +- HashAggregate(keys=[customer#35], functions=[sum(sales#37L)])\n",
      "                           +- Exchange hashpartitioning(customer#35, 1000), ENSURE_REQUIREMENTS, [plan_id=564]\n",
      "                              +- HashAggregate(keys=[customer#35], functions=[partial_sum(sales#37L)])\n",
      "                                 +- Project [customer#35, sales#37L]\n",
      "                                    +- Scan ExistingRDD[customer#35,product#36,sales#37L]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|max_sales|\n",
      "+---------+\n",
      "|      630|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"CacheShufflingExample\").getOrCreate()\n",
    "\n",
    "# Sample Data (simulating sales data)\n",
    "data = [\n",
    "    (\"A\", \"Product1\", 100),\n",
    "    (\"B\", \"Product2\", 200),\n",
    "    (\"A\", \"Product3\", 150),\n",
    "    (\"C\", \"Product1\", 50),\n",
    "    (\"B\", \"Product2\", 250),\n",
    "    (\"A\", \"Product1\", 120),\n",
    "    (\"C\", \"Product3\", 80),\n",
    "    (\"B\", \"Product1\", 180),\n",
    "    (\"A\", \"Product2\", 90)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"customer\", \"product\", \"sales\"])\n",
    "\n",
    "# --- Without Caching ---\n",
    "print(\"Without Caching:\")\n",
    "\n",
    "# First aggregation (sales per customer)\n",
    "customer_sales = df.groupBy(\"customer\").agg(sum(\"sales\").alias(\"total_sales\"))\n",
    "customer_sales.explain()\n",
    "customer_sales.show()\n",
    "\n",
    "# Second aggregation (average sales per customer)\n",
    "average_sales = customer_sales.agg(avg(\"total_sales\").alias(\"average_sales\"))\n",
    "average_sales.explain()\n",
    "average_sales.show()\n",
    "\n",
    "# Third aggregation (max sales per customer)\n",
    "max_sales = customer_sales.agg(max(\"total_sales\").alias(\"max_sales\"))\n",
    "max_sales.explain()\n",
    "max_sales.show()\n",
    "\n",
    "# --- With Caching ---\n",
    "print(\"\\nWith Caching:\")\n",
    "\n",
    "# First aggregation (sales per customer) and CACHING\n",
    "customer_sales_cached = df.groupBy(\"customer\").agg(sum(\"sales\").alias(\"total_sales\")).cache()\n",
    "customer_sales_cached.explain()\n",
    "customer_sales_cached.show()\n",
    "\n",
    "# Second aggregation (average sales per customer) - uses the CACHED data\n",
    "average_sales_cached = customer_sales_cached.agg(avg(\"total_sales\").alias(\"average_sales\"))\n",
    "average_sales_cached.explain()\n",
    "average_sales_cached.show()\n",
    "\n",
    "# Third aggregation (max sales per customer) - uses the CACHED data\n",
    "max_sales_cached = customer_sales_cached.agg(max(\"total_sales\").alias(\"max_sales\"))\n",
    "max_sales_cached.explain()\n",
    "max_sales_cached.show()\n",
    "\n",
    "# Unpersist the cached DataFrame to release memory\n",
    "customer_sales_cached.unpersist()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4331e78-546a-4e89-95cc-ba55a8563768",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
