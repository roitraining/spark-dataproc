CLUSTER_NAME=dataproc-cluster
PROJECT_ID=$(gcloud config get-value project)
PROJECT_NUMBER=$(gcloud projects describe $PROJECT_ID --format="value(projectNumber)")
REGION=us-central1
echo $PROJECT_ID $PROJECT_NUMBER $REGION

gcloud storage buckets create gs://${PROJECT_ID}-data --location=$REGION
gcloud storage buckets create gs://${PROJECT_ID}-output --location=$REGION

gsutil cp gs://goog-dataproc-initialization-actions-$REGION/python/pip-install.sh gs://${PROJECT_ID}-data/pip-install.sh

gcloud services disable metastore.googleapis.com
gcloud services disable dataproc.googleapis.com
gcloud services enable dataproc.googleapis.com
gcloud services enable metastore.googleapis.com

gcloud metastore services create dataproc-metastore \
    --location=$REGION \
    --network=projects/$PROJECT_ID/global/networks/default \
    --database-type=SPANNER \
    --release-channel=STABLE \
    --autoscaling-enabled

METASTORE_URI=$(gcloud metastore services describe dataproc-metastore --location=$REGION --format="value(endpointUri)")
echo $METASTORE_URI

gcloud projects add-iam-policy-binding $PROJECT_ID --member="serviceAccount:${PROJECT_NUMBER}-compute@developer.gserviceaccount.com" --role="roles/storage.admin"

gcloud compute networks subnets update default --enable-private-ip-google-access --project "$PROJECT_ID" --region $REGION

# Create cluster with Public IP
gcloud dataproc clusters create $CLUSTER_NAME --enable-component-gateway --region us-central1 --master-machine-type n2-standard-4 --master-boot-disk-type pd-balanced --master-boot-disk-size 100 --num-workers 4 --worker-machine-type n2-standard-4 --worker-boot-disk-type pd-balanced --worker-boot-disk-size 100 --image-version 2.2-debian12 --optional-components JUPYTER,DOCKER,HUDI --dataproc-metastore projects/$PROJECT_ID/locations/us-central1/services/dataproc-metastore --project $PROJECT_ID --public-ip-address \
--properties spark:spark.jars.packages=org.apache.spark:spark-avro_2.12:3.5.0 \
--initialization-actions  gs://${PROJECT_ID}-data/pip-install.sh \
--metadata PIP_PACKAGES="sparksql-magic pandas matplotlib"


# Create cluster with Private IP
gcloud dataproc clusters create $CLUSTER_NAME --enable-component-gateway --region us-central1 --master-machine-type n2-standard-4 --master-boot-disk-type pd-balanced --master-boot-disk-size 100 --num-workers 4 --worker-machine-type n2-standard-4 --worker-boot-disk-type pd-balanced --worker-boot-disk-size 100 --image-version 2.2-debian12 --optional-components JUPYTER,DOCKER,HUDI --dataproc-metastore projects/$PROJECT_ID/locations/us-central1/services/dataproc-metastore --project $PROJECT_ID --no-address \
--properties spark:spark.jars.packages=org.apache.spark:spark-avro_2.12:3.5.0 \
--initialization-actions  gs://${PROJECT_ID}-data/pip-install.sh \
--metadata PIP_PACKAGES="sparksql-magic pandas matplotlib"

gsutil uniformbucketlevelaccess set on gs://dataproc-staging-
